{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d5f9aab-8560-4434-98a9-f8327f22b8d0",
   "metadata": {},
   "source": [
    "# Mazu Talk\n",
    "Mazu Talk is a GPT style, Transformer based Decoder. The code is adapted from two sources:\n",
    "* the [GPT tutorial](https://keras.io/examples/generative/text_generation_with_miniature_gpt/) by Apoorv Nandan available on the Keras website.\n",
    "* Generative Deep Learning, 2nd edition, by David Foster (O’Reilly), 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befa48ea-8500-48c6-be1a-47c374183979",
   "metadata": {},
   "source": [
    "## Install libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aa511d-3526-4dbd-aa47-5dec53f83643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U deep-translator\n",
    "# !poetry add deep-translator   # for poetry usage\n",
    "\n",
    "# from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f479a8dd-2828-436b-a365-63ee8a3d5e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-07 21:59:05.568574: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses, callbacks, saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8cfd20b-1277-48cf-8f78-2eabd62abb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set min. log level for TF to mute warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f53356-c960-4be6-830a-ee35ffcb24dc",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2080df5-113d-48ca-888c-3dee16292197",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 50000\n",
    "# MAX_LEN = 80\n",
    "MAX_LEN = 80\n",
    "EMBEDDING_DIM = 256\n",
    "KEY_DIM = 256\n",
    "N_HEADS = 2\n",
    "FEED_FORWARD_DIM = 256\n",
    "VALIDATION_SPLIT = 0.2\n",
    "SEED = 42\n",
    "LOAD_MODEL = False\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "DATASET_REPETITIONS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f1c42f-35a4-4f68-9481-a02d1fa96b76",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "Chinese Poems are sourced from:\n",
    "* https://www.kaggle.com/datasets/qianboao/chinesepoetrydataset\n",
    "* https://github.com/chinese-poetry/chinese-poetry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8f5b62-1319-4622-816e-f9920c0c537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file containing Chinese poetry\n",
    "# with open('/app/data/chinese-poetry/chinese_poems.txt', 'r') as f:\n",
    "#     zh_poems = f.readlines()\n",
    "    \n",
    "# print(zh_poems[:5])\n",
    "# print(len(zh_poems))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec41c58-9be9-4b05-89e8-8a8e6e503599",
   "metadata": {},
   "source": [
    "### Translate Chinese poems to Swedish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065cfcad-1cc9-4966-9e01-536a23d39f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Google Translator\n",
    "translator = GoogleTranslator(source='zh-CN', target='sv')\n",
    "\n",
    "# Step through the Chinese poems in batches of 1000 poems each\n",
    "# TODO: start by testing a smaller set of batches, then get the rest\n",
    "# for i in range(5000, 305000, 1000):\n",
    "for i in range(0, 5000, 1000):\n",
    "    print(f\"Translating batch {i}\")\n",
    "    # Create list to store the translations in\n",
    "    zh_poems_sv = []\n",
    "    for poem in zh_poems[i: i + 1000]:\n",
    "        try:\n",
    "            # Send a batch to the translator and append to the above list\n",
    "            zh_poems_sv.append(translator.translate(poem))\n",
    "        except:\n",
    "            print(\"Error: Could not translate a poem.\")\n",
    "    # Save the batch as a json file\n",
    "    with open(\"./zh_poems_sv/zh_poems_sv_%000006d_%000006d.json\" % (i, i + 1000), 'w') as f:\n",
    "        json.dump(zh_poems_sv, f)\n",
    "    print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f5b5dd-17ca-42c7-8a29-ef67c506c549",
   "metadata": {},
   "source": [
    "### Load Swedish translations from saved files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17a1101f-3536-444e-b1e0-c1a8f6bb82f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 files\n",
      "Found 5000 poems\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Vem känner inte till våren när en tjänsteman degraderas? Han kan fortfarande vara full efter att ha lämnat Guo. Silverpennan jagar Bao Xie och Ximen skriver en mening för att imitera molnet. Dongyuan vågar läsa Bai Pengxi och Nanmu borde arbeta med tusentals par. Det finns nya dikter kvar att tigga. Jag är inte alls ond, jag är rädd att jag hörs över hela himlen genom att slå på mitt horn.',\n",
       " 'Glaset utanför bambun är tio hektar brett, med glaserade plattor ristade högt och lågt. Höstvinden blåser genom den kalldoftande jianjian, ensam och vacker, den svala månen är kall i gryningen. Den glada atmosfären är lika hög som att gå ut av samhället, men vem kan se charmen och sederna hos Yi. Jag skämtar om att jag kysser min moster idag, och Taihua sjunger högt. Han räknas inte med.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all the files\n",
    "file_list = glob.glob(\"/app/data/zh_poems_sv/*.json\")\n",
    "print(f\"Found {len(file_list)} files\")\n",
    "file_list\n",
    "\n",
    "# Put the file contents in a list\n",
    "translations_sv = []\n",
    "for file in file_list:\n",
    "    with open(file, 'r') as f:\n",
    "        for poem in json.load(f):\n",
    "            translations_sv.append(poem)\n",
    "\n",
    "# Print some examples of the list\n",
    "print(f\"Found {len(translations_sv)} poems\")\n",
    "translations_sv[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae2f5f4-d9fc-44aa-aecc-c05a507c6302",
   "metadata": {},
   "source": [
    "### Concatenate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77c28a08-5e9f-4c77-a4b2-f5da81915660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_data = translations_sv\n",
    "len(complete_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe1572-f4f6-46a6-8bb2-fdbbc7165be2",
   "metadata": {},
   "source": [
    "## Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c83d0357-7def-4f52-85b7-df4eb363a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the punctuation, to treat them as separate 'words'\n",
    "def pad_punctuation(s):\n",
    "    s = re.sub(f\"([{string.punctuation}, '\\n'])\", r\" \\1 \", s)\n",
    "    s = re.sub(\" +\", \" \", s)\n",
    "    return s\n",
    "\n",
    "text_data = [pad_punctuation(x) for x in complete_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89850727-5608-4622-961e-7c46e9a30fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Den utsökta målarpaviljongen går in i Hongming och den vaga Hongmingen går in i Taiqing . Den lila luften rör sig bort , och de gröna molnen och dimman lyser steg för steg . Vågorna av persikoblomningar tränger igenom tre berg och sköldpaddshornsskärmen är sju hög Plommonfen är utvisad från den jordiska världen , Vem visste att det fanns Peng Ying i världen ? '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display an example of a recipe\n",
    "example_data = text_data[25]\n",
    "example_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f38b5321-379f-4abe-8812-63ca97b11238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a Tensorflow Dataset\n",
    "text_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices(text_data)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .shuffle(1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "908b0ada-4ade-4e04-99b4-fc77bf19f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vectorisation layer\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=\"lower\",\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=MAX_LEN + 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88a3f0b3-3ffa-4351-9c40-21e02e7ac183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-07 22:01:20.126113: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Adapt the layer to the training set\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f93b83ea-5efa-4603-a4f4-23105b4f0b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: \n",
      "1: [UNK]\n",
      "2: och\n",
      "3: .\n",
      "4: ,\n",
      "5: är\n",
      "6: att\n",
      "7: i\n",
      "8: det\n",
      "9: jag\n"
     ]
    }
   ],
   "source": [
    "# Display some token:word mappings\n",
    "for i, word in enumerate(vocab[:10]):\n",
    "    print(f\"{i}: {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9dcb9fd-7a27-4d11-812e-0d5c3c7e3de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   11  2411 17704    60    67     7 10323     2    11  4257 20577    60\n",
      "    67     7  8788     3    11   274   459   253    30    74     4     2\n",
      "    12    51    40     2   374   106   692    16   692     3   202    13\n",
      " 16883  2132   747    53   243     2 15063     5   985   265  9414     5\n",
      "  6669    32    11  3373    48     4    73   680     6     8   369  1545\n",
      "  1179     7    48    37     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# Display the same example converted to ints\n",
    "example_tokenised = vectorize_layer(example_data)\n",
    "print(example_tokenised.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95833ed6-0193-4a04-9edd-f05efed316a1",
   "metadata": {},
   "source": [
    "## Create the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d21a501-f5ec-4cd8-87b1-a0625e479429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training set of recipes and the same text shifted by one word\n",
    "def prepare_inputs(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# train_ds = text_ds.map(prepare_inputs)\n",
    "train_ds = text_ds.map(prepare_inputs).repeat(DATASET_REPETITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30ec8392-1daf-4ded-9169-6c57ccd4e464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(80,), dtype=int64, numpy=\n",
       "array([10446,     5,  1144,     2,  2279,     2,  1451,     5,   100,\n",
       "          88,    32,  7000,     3,  7129,   324,     5,    46,  1111,\n",
       "          20,  6256,     2,  6813,     5,    20,   288,  9429, 19231,\n",
       "         853,    19,    68,     4,     2,     8,    27,  5300,  3790,\n",
       "           4,     2,    12,  6250,  7770,  2870,  1509, 22717,     8,\n",
       "          27,  1509,  6225,     7,  4720,     4,   996,   996,    21,\n",
       "          25,     6,   220,   219,    85,   329,    37,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_output = train_ds.take(1).get_single_element()\n",
    "# Example Input\n",
    "example_input_output[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8364903d-12c0-4b45-9496-3015748f3601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(80,), dtype=int64, numpy=\n",
       "array([    5,  1144,     2,  2279,     2,  1451,     5,   100,    88,\n",
       "          32,  7000,     3,  7129,   324,     5,    46,  1111,    20,\n",
       "        6256,     2,  6813,     5,    20,   288,  9429, 19231,   853,\n",
       "          19,    68,     4,     2,     8,    27,  5300,  3790,     4,\n",
       "           2,    12,  6250,  7770,  2870,  1509, 22717,     8,    27,\n",
       "        1509,  6225,     7,  4720,     4,   996,   996,    21,    25,\n",
       "           6,   220,   219,    85,   329,    37,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0])>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Output (shifted by one token)\n",
    "example_input_output[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd31aef0-9c23-4f09-9337-087b421c5223",
   "metadata": {},
   "source": [
    "## Create the Causal Attention Mask function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "492308d1-a864-401c-92b7-3a008f5e3817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "np.transpose(causal_attention_mask(1, 10, 10, dtype=tf.int32)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b2e098-9362-4aa2-862d-e19c85e04ecb",
   "metadata": {},
   "source": [
    "## Create a Transformer Block layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ea66107-52d3-48d3-aee0-987c08841c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim, embed_dim, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.attn = layers.MultiHeadAttention(\n",
    "            num_heads, key_dim, output_shape=embed_dim\n",
    "        )\n",
    "        self.dropout_1 = layers.Dropout(self.dropout_rate)\n",
    "        self.ln_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ffn_1 = layers.Dense(self.ff_dim, activation=\"relu\")\n",
    "        self.ffn_2 = layers.Dense(self.embed_dim)\n",
    "        self.dropout_2 = layers.Dropout(self.dropout_rate)\n",
    "        self.ln_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(\n",
    "            batch_size, seq_len, seq_len, tf.bool\n",
    "        )\n",
    "        attention_output, attention_scores = self.attn(\n",
    "            inputs,\n",
    "            inputs,\n",
    "            attention_mask=causal_mask,\n",
    "            return_attention_scores=True,\n",
    "        )\n",
    "        attention_output = self.dropout_1(attention_output)\n",
    "        out1 = self.ln_1(inputs + attention_output)\n",
    "        ffn_1 = self.ffn_1(out1)\n",
    "        ffn_2 = self.ffn_2(ffn_1)\n",
    "        ffn_output = self.dropout_2(ffn_2)\n",
    "        return (self.ln_2(out1 + ffn_output), attention_scores)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"key_dim\": self.key_dim,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "                \"ff_dim\": self.ff_dim,\n",
    "                \"dropout_rate\": self.dropout_rate,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc16c72-29c4-4e32-b68e-353d414d0d81",
   "metadata": {},
   "source": [
    "## Create Token and Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "021887e2-9049-4530-98e9-f3de4ab4b7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, max_len, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.token_emb = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.pos_emb = layers.Embedding(input_dim=max_len, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"max_len\": self.max_len,\n",
    "                \"vocab_size\": self.vocab_size,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db7f2dd-0cca-469b-947f-717e0b57514e",
   "metadata": {},
   "source": [
    "## Build the Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44e0f453-1bee-4761-8b92-dee8f7a423a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
    "x = TokenAndPositionEmbedding(MAX_LEN, VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
    "x, attention_scores = TransformerBlock(\n",
    "    N_HEADS, KEY_DIM, EMBEDDING_DIM, FEED_FORWARD_DIM\n",
    ")(x)\n",
    "outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "gpt = models.Model(inputs=inputs, outputs=[outputs, attention_scores])\n",
    "gpt.compile(\"adam\", loss=[losses.SparseCategoricalCrossentropy(), None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c53d0cb0-e0af-47ed-8524-240b12d4b12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ token_and_position_embedding    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,820,480</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block               │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">658,688</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)] │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50000</span>)    │    <span style=\"color: #00af00; text-decoration-color: #00af00\">12,850,000</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ token_and_position_embedding    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │    \u001b[38;5;34m12,820,480\u001b[0m │\n",
       "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block               │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),    │       \u001b[38;5;34m658,688\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)] │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50000\u001b[0m)    │    \u001b[38;5;34m12,850,000\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,329,168</span> (100.44 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26,329,168\u001b[0m (100.44 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,329,168</span> (100.44 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,329,168\u001b[0m (100.44 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8d928b0-a0c3-4c68-86f7-3a0472e0a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_MODEL:\n",
    "    gpt.load_weights(\"./checkpoint/checkpoint.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d911e71-4825-4e49-b563-8fe860f42d67",
   "metadata": {},
   "source": [
    "## Train the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7900628a-2f20-43ac-ab41-7079f0ab88e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TextGenerator checkpoint\n",
    "class TextGenerator(callbacks.Callback):\n",
    "    def __init__(self, index_to_word, top_k=10):\n",
    "        self.index_to_word = index_to_word\n",
    "        self.word_to_index = {\n",
    "            word: index for index, word in enumerate(index_to_word)\n",
    "        }\n",
    "\n",
    "    def sample_from(self, probs, temperature):\n",
    "        probs = probs ** (1 / temperature)\n",
    "        probs = probs / np.sum(probs)\n",
    "        return np.random.choice(len(probs), p=probs), probs\n",
    "\n",
    "    def generate(self, start_prompt, max_tokens, temperature):\n",
    "        start_tokens = [\n",
    "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
    "        ]\n",
    "        sample_token = None\n",
    "        info = []\n",
    "        while len(start_tokens) < max_tokens and sample_token != 0:\n",
    "            x = np.array([start_tokens])\n",
    "            y, att = self.model.predict(x, verbose=0)\n",
    "            sample_token, probs = self.sample_from(y[0][-1], temperature)\n",
    "            info.append(\n",
    "                {\n",
    "                    \"prompt\": start_prompt,\n",
    "                    \"word_probs\": probs,\n",
    "                    \"atts\": att[0, :, -1, :],\n",
    "                }\n",
    "            )\n",
    "            start_tokens.append(sample_token)\n",
    "            start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
    "        print(f\"\\ngenerated text:\\n{start_prompt}\\n\")\n",
    "        return info\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.generate(\"Vatten och luft\", max_tokens=80, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60a93904-cdcc-465a-98da-533cf8ec3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model save checkpoint\n",
    "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=\"./checkpoint/checkpoint..weights.h5\",\n",
    "    save_weights_only=True,\n",
    "    save_freq=\"epoch\",\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "tensorboard_callback = callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "\n",
    "# Tokenize starting prompt\n",
    "text_generator = TextGenerator(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cbf863ea-c126-4f8c-b44c-3d0d9299aaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1712527518.932882   12254 service.cc:145] XLA service 0x7fd940019cb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1712527518.932917   12254 service.cc:153]   StreamExecutor device (0): NVIDIA RTX A2000 8GB Laptop GPU, Compute Capability 8.6\n",
      "2024-04-07 22:05:18.981426: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "W0000 00:00:1712527519.075204   12254 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2024-04-07 22:05:19.207731: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8906\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1712527520.975316   12430 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_45', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527522.320053   12421 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_17', 156 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527522.542387   12426 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 304 bytes spill stores, 304 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527522.624903   12432 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_46', 192 bytes spill stores, 160 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527522.955124   12419 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 100 bytes spill stores, 92 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527522.993686   12420 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_41', 20 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527524.310182   12435 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_17', 404 bytes spill stores, 376 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527524.387765   12434 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 88 bytes spill stores, 88 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527524.607153   12431 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527525.334837   12432 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527525.658978   12419 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527525.881914   12433 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 672 bytes spill stores, 640 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527525.924034   12426 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527526.114353   12431 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527526.445552   12429 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527526.609087   12434 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527526.613687   12430 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_32', 184 bytes spill stores, 184 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527527.004248   12432 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_7', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527527.121733   12430 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_42', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527527.203836   12437 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_32', 204 bytes spill stores, 160 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527535.861462   12254 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m112/785\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 56ms/step - loss: 6.9594"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1712527542.699245   12255 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "I0000 00:00:1712527544.301802   12778 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527544.337001   12767 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_42', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527545.020795   12777 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527545.132525   12785 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527546.219151   12770 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527546.612399   12781 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_17', 276 bytes spill stores, 276 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527546.988278   12768 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_32', 260 bytes spill stores, 268 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527548.117974   12776 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_41', 144 bytes spill stores, 144 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527548.362330   12777 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_33', 172 bytes spill stores, 176 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527548.593149   12783 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_41', 404 bytes spill stores, 404 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527548.615506   12785 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 304 bytes spill stores, 304 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527548.637595   12772 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_41', 316 bytes spill stores, 308 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527548.709685   12784 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 672 bytes spill stores, 640 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527548.839284   12771 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_7', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527548.880169   12772 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_41', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527549.108496   12780 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 88 bytes spill stores, 88 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527549.373149   12776 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_7', 28 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527549.674127   12768 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_33', 96 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527550.012946   12783 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 148 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527550.288678   12779 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m785/785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 4.7119"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1712527600.157666   13367 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527604.883793   13661 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_7', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527605.875018   13759 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527606.311366   13758 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_7', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527607.535629   13852 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_7', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527608.895134   13952 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_7', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527609.213050   13948 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527610.731780   14057 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_7', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527611.828671   14157 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_7', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527612.126562   14157 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527613.292685   14248 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_7', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527616.562026   14447 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527618.039703   14563 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527620.188378   14659 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527621.621280   14781 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527623.287987   14874 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527623.319422   14877 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527623.908673   14883 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527625.211379   14983 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527625.248784   14999 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527625.747531   14989 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527627.265280   15103 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527627.407503   15101 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527627.495859   15089 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527628.758235   15197 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527628.779036   15211 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527628.792944   15206 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527630.932395   15314 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527631.155800   15316 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527632.846187   15415 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527632.863020   15431 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527634.467351   15530 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527634.784037   15530 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527636.473979   15641 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527636.523660   15639 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527637.987753   15748 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527639.966369   15862 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527641.779460   15964 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527643.598502   16070 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527645.622907   16177 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527650.426642   16416 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527651.360315   16430 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527653.194859   16534 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 164 bytes spill stores, 164 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527655.898600   16657 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527656.837771   16668 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527658.458492   16786 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527661.383748   16902 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527661.856094   16904 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527664.669732   17034 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527664.885209   17031 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 164 bytes spill stores, 164 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527667.452260   17157 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527670.350312   17285 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527672.758734   17392 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527678.341648   17647 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527683.985810   17888 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527689.176712   18115 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 100 bytes spill stores, 100 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527689.656986   18119 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527691.926473   18253 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 104 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527695.533575   18365 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 100 bytes spill stores, 100 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527698.816175   18492 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 256 bytes spill stores, 272 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527701.759537   18618 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527704.339554   18732 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 12 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527707.019983   18843 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527707.916135   18848 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527710.173246   18976 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 256 bytes spill stores, 272 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527711.066106   18979 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527722.611633   19458 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527730.546984   19823 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 100 bytes spill stores, 100 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527733.113474   19943 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 20 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527736.565945   20055 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527736.953942   20068 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527737.296551   20062 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 456 bytes spill stores, 264 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527737.330967   20074 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527737.722188   20057 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527740.521932   20192 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 104 bytes spill stores, 108 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527740.768130   20196 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 52 bytes spill stores, 52 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527741.496292   20185 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 448 bytes spill stores, 260 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527741.712096   20194 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527744.640075   20315 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527744.693045   20334 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 96 bytes spill stores, 96 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527745.444457   20315 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 56 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527746.018014   20320 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "generated text:\n",
      "Vatten och luft har smakar på dagen när man ser orden . den vita yao - gamla seden är full av höst . blommorna är smärtsamt att månens själ , och täckt av doftande vinden och tomma floden miluo i fara är lätta goda hästar och flyger av röda fisken och redo att göra dig om liu lågor . vinden öste ner på musiken i snön . \n",
      "\n",
      "\u001b[1m785/785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 270ms/step - loss: 4.7107\n",
      "Epoch 2/5\n",
      "\u001b[1m785/785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 2.3364"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1712527795.591746   22043 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 208 bytes spill stores, 208 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527795.910365   22036 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 32 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527796.119441   22051 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527796.195171   22042 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527797.111787   22047 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 984 bytes spill stores, 1308 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527799.874989   22170 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 80 bytes spill stores, 80 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527800.196303   22169 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527800.206739   22173 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527801.115388   22177 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527801.133595   22167 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527803.865016   22301 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 104 bytes spill stores, 108 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527804.224017   22297 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527804.255887   22300 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 480 bytes spill stores, 276 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527804.610374   22308 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 88 bytes spill stores, 88 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527805.465538   22297 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 56 bytes spill stores, 56 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527808.553414   22429 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 44 bytes spill stores, 40 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527809.093027   22442 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527809.376588   22434 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 80 bytes spill stores, 80 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527809.478555   22428 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527809.486680   22444 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527812.505038   22562 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 984 bytes spill stores, 1308 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527812.602821   22574 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 208 bytes spill stores, 208 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527812.961259   22564 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 68 bytes spill stores, 68 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527813.079440   22577 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 72 bytes spill stores, 72 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712527813.099022   22562 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "generated text:\n",
      "Vatten och luft har gett pärlor subtila kang blivit mer än att se fram emot bröstet . han tittar på många sjukdomar ber om himlens port , och en vårutflykt kan resa utan bambu , om hästen planteras sparsamt för dag . du kan inte med dig lycklig blir du kan inte röra dig i skymningen . det är inte lätt , om du tänker utan pengar på de tre år . \n",
      "\n",
      "\u001b[1m785/785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 85ms/step - loss: 2.3361\n",
      "Epoch 3/5\n",
      "\u001b[1m785/785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1.0863\n",
      "generated text:\n",
      "Vatten och luft har fallit är lång , och den sorglösa i klipprötterna är flitig och stenen är oregerlig , men hängande . vad konstigt att tänka på den till shaoyang börjar , och det är särskilt märkligt . \n",
      "\n",
      "\u001b[1m785/785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 58ms/step - loss: 1.0860\n",
      "Epoch 4/5\n",
      "\u001b[1m785/785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.5210\n",
      "generated text:\n",
      "Vatten och luft har hundra år \" hänger fortfarande gäster , och den vita snön är alltid tom . solen är trötta och tyst , och den gamla stjärnan återvänder från zhengshi . molnen på båda ändar . molnen verkar vara olika , och det kommer och regnet att sjunga guo , och det finns en plats att hugga ved kan skördas i den vidare hastigt . \n",
      "\n",
      "\u001b[1m785/785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 60ms/step - loss: 0.5209\n",
      "Epoch 5/5\n",
      "\u001b[1m785/785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 0.3228\n",
      "generated text:\n",
      "Vatten och luft hundra fot långa människor har gett mig ben . jag är fullt av energi för att odla världen . jag är nöjd med dig mitt liv i shaoyang , och jag är också och mitt liv och ge upp ett kort , som du kan se det gamla sköldpaddsboet lotus krymper , spelet är också mycket färgstarkt . \n",
      "\n",
      "\u001b[1m785/785\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 60ms/step - loss: 0.3228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fda46930090>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[model_checkpoint_callback, tensorboard_callback, text_generator],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3a6256-488e-4c64-929a-c288c377b228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
