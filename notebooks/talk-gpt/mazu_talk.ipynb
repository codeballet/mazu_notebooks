{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d5f9aab-8560-4434-98a9-f8327f22b8d0",
   "metadata": {},
   "source": [
    "# Mazu Talk\n",
    "Mazu Talk is a GPT style, Transformer based Decoder. The code is adapted from two sources:\n",
    "* the [GPT tutorial](https://keras.io/examples/generative/text_generation_with_miniature_gpt/) by Apoorv Nandan available on the Keras website.\n",
    "* Generative Deep Learning, 2nd edition, by David Foster (O’Reilly), 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befa48ea-8500-48c6-be1a-47c374183979",
   "metadata": {},
   "source": [
    "## Install libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4aa511d-3526-4dbd-aa47-5dec53f83643",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U deep-translator\n",
    "!poetry add deep-translator   # for poetry usage\n",
    "!pip install gTTS\n",
    "\n",
    "from deep_translator import GoogleTranslator\n",
    "from gtts import gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f479a8dd-2828-436b-a365-63ee8a3d5e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-13 06:23:36.120555: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras import layers, models, losses, callbacks, saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8cfd20b-1277-48cf-8f78-2eabd62abb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set min. log level for TF to mute warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f53356-c960-4be6-830a-ee35ffcb24dc",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2080df5-113d-48ca-888c-3dee16292197",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 100000\n",
    "MAX_LEN = 80\n",
    "# EMBEDDING_DIM = 256\n",
    "EMBEDDING_DIM = 512\n",
    "# KEY_DIM = 256\n",
    "KEY_DIM = 512\n",
    "N_HEADS = 4\n",
    "# FEED_FORWARD_DIM = 256\n",
    "FEED_FORWARD_DIM = 512\n",
    "VALIDATION_SPLIT = 0.2\n",
    "SEED = 42\n",
    "LOAD_MODEL = False\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "DATASET_REPETITIONS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f1c42f-35a4-4f68-9481-a02d1fa96b76",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "Chinese Poems are sourced from:\n",
    "* https://www.kaggle.com/datasets/qianboao/chinesepoetrydataset\n",
    "* https://github.com/chinese-poetry/chinese-poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fc730e-80c1-4812-832e-1da30e668f2c",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61957c4d-110b-4521-86c6-62ee1a4767b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean a text from special characters\n",
    "def clean_text(text):\n",
    "    # Remove content within brackets\n",
    "    pattern_brackets = r'\\(.*?\\)'\n",
    "    cleaned_text = re.sub(pattern_brackets, '', text)\n",
    "    \n",
    "    # Remove newline characters (\\n) and tab characters (\\t)\n",
    "    cleaned_text = cleaned_text.replace('\\n', '').replace('\\t', '')\n",
    "    \n",
    "    # Replace hyphens with whitespace\n",
    "    cleaned_text = cleaned_text.replace('-', ' ')\n",
    "    \n",
    "    # Remove curly double quotes\n",
    "    cleaned_text = cleaned_text.replace(\"“\", '').replace(\"”\", '')\n",
    "\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbf0b4d-0e11-484c-b3f2-6c9e2c7c2019",
   "metadata": {},
   "source": [
    "### Load Chinese poems from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8f5b62-1319-4622-816e-f9920c0c537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file containing Chinese poetry\n",
    "with open('/app/data/chinese-poetry/chinese_poems.txt', 'r') as f:\n",
    "    zh_poems = f.readlines()\n",
    "    \n",
    "print(zh_poems[:5])\n",
    "print(len(zh_poems))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d71e16e-1e42-4d06-8ccd-4850a8c415e7",
   "metadata": {},
   "source": [
    "### Translate Chinese poems to Swedish and save as files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829c1a70-ccc7-4cb9-9fb4-ea518fbffcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated workflow for translating Chinese poems\n",
    "\n",
    "# Instantiate the Google Translator\n",
    "translator = GoogleTranslator(source='zh-CN', target='sv')\n",
    "\n",
    "for i in range(7000, 8000, 1000):\n",
    "    print(f\"translating batch {i}\")\n",
    "    # Create a List to store the translations in\n",
    "    zh_poems_sv = []\n",
    "    counter = 0\n",
    "\n",
    "    for poem in zh_poems[i: i + 1000]:\n",
    "        # Inform progress\n",
    "        if counter % 200 == 0:\n",
    "            print(f\"{counter} translations done!\")\n",
    "        counter += 1\n",
    "        # Clean the text\n",
    "        poem = clean_text(poem)\n",
    "        try:\n",
    "            # Send a batch to the translator and append to the above list\n",
    "            zh_poems_sv.append(translator.translate(poem))\n",
    "        except:\n",
    "            print(\"Error: Could not translate a poem.\")\n",
    "    # Save the batch as a json file\n",
    "    with open(\"/app/data/zh_poems_sv/zh_poems_sv_%000006d_%000006d.json\" % (i, i + 1000), 'w') as f:\n",
    "        json.dump(zh_poems_sv, f)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f5b5dd-17ca-42c7-8a29-ef67c506c549",
   "metadata": {},
   "source": [
    "### Load Swedish translations of Chinese poems from saved files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17a1101f-3536-444e-b1e0-c1a8f6bb82f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 files\n",
      "Found 8000 poems\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Vem känner inte till våren när en tjänsteman degraderas? Han kan fortfarande vara full efter att ha lämnat Guo. Silverpennan jagar Bao Xie och Ximen skriver en mening för att imitera molnet. Dongyuan vågar läsa Bai Pengxi och Nanmu borde arbeta med tusentals par. Det finns nya dikter kvar att tigga. Jag är inte alls ond, jag är rädd att jag hörs över hela himlen genom att slå på mitt horn.',\n",
       " 'Glaset utanför bambun är tio hektar brett, med glaserade plattor ristade högt och lågt. Höstvinden blåser genom den kalldoftande jianjian, ensam och vacker, den svala månen är kall i gryningen. Den glada atmosfären är lika hög som att gå ut av samhället, men vem kan se charmen och sederna hos Yi. Jag skämtar om att jag kysser min moster idag, och Taihua sjunger högt. Han räknas inte med.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find all the files\n",
    "file_list = glob.glob(\"/app/data/zh_poems_sv/*.json\")\n",
    "print(f\"Found {len(file_list)} files\")\n",
    "file_list\n",
    "\n",
    "# Put the file contents in a list\n",
    "translations_sv = []\n",
    "for file in file_list:\n",
    "    with open(file, 'r') as f:\n",
    "        for poem in json.load(f):\n",
    "            translations_sv.append(poem)\n",
    "\n",
    "# Print some examples of the list\n",
    "print(f\"Found {len(translations_sv)} poems\")\n",
    "translations_sv[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06512e7b-4958-4bf4-9294-487aa6a190ee",
   "metadata": {},
   "source": [
    "### Load PhD, English version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccef935-3223-4f2c-a383-8c86e2a61fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/app/data/stjernholm-texts/phd_thesis.txt', 'r') as f:\n",
    "    phd = f.readlines()\n",
    "phd[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d280e8-068b-40dc-9df7-289a4cc40df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove headings and short paragraphs\n",
    "def filter_long_strings(input_list, min_length=40):\n",
    "    \"\"\"\n",
    "    Removes strings from the input list that have a length less than min_length.\n",
    "    Args:\n",
    "        input_list (list): List of strings.\n",
    "        min_length (int, optional): Minimum length for strings to keep. Defaults to 40.\n",
    "\n",
    "    Returns:\n",
    "        list: Filtered list containing only strings with length greater than or equal to min_length.\n",
    "    \"\"\"\n",
    "    return [string for string in input_list if len(string) > min_length]\n",
    "\n",
    "# Example usage:\n",
    "shorter_phd = filter_long_strings(phd)\n",
    "shorter_phd[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6793b7d1-a404-405b-8d21-3707bf669759",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_phd = [clean_text(x) for x in shorter_phd]\n",
    "cleaned_phd[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0860b9-ffb5-49d6-8779-e7d721af1422",
   "metadata": {},
   "source": [
    "### Translate PhD to Swedish and save as files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474faa28-125a-4fd5-b34c-2ad9702e45bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated workflow for translating PhD to Swedish\n",
    "\n",
    "# Instantiate the Google Translator\n",
    "translator = GoogleTranslator(source='en', target='sv')\n",
    "\n",
    "# Create a List to store the translations in\n",
    "phd_sv = []\n",
    "\n",
    "for section in cleaned_phd:\n",
    "    # Clean the text\n",
    "    section = clean_text(section)\n",
    "    try:\n",
    "        # Send a batch to the translator and append to the above list\n",
    "        phd_sv.append(translator.translate(section))\n",
    "    except:\n",
    "        print(\"Error: Could not translate a section.\")\n",
    "        \n",
    "# Save as a json file\n",
    "with open(\"/app/data/stjernholm-texts/phd_thesis_sv.json\", \"w\") as f:\n",
    "    json.dump(phd_sv, f)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c81d734-4118-4949-9b88-10c2af532a09",
   "metadata": {},
   "source": [
    "### Load PhD Swedish version from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e8e2371-7226-467d-ab03-7a6c1b9e6a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "603\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Denna avhandling undersöker sambanden mellan subjektiv perception och dansrörelser. Dessa primära relationer innebär i sin tur ett brett spektrum av tvärvetenskapliga sekundära relationer, relaterade till förkroppsligande, minne, prestation, subjektivitet, olika former av representation, medling, observation, teori och koreografiska praktiker. Uppfattningen i fråga hänvisar till ett fenomenologiskt inducerat tillvägagångssätt, huvudsakligen härrörande från Ernst Cassirer, Maurice Merleau Ponty och Gaston Bachelard. Min tillämpning av fenomenologisk teori kännetecknas vidare av en särskild läsning av de tre begreppen performativitet, virtualitet och abstraktion. Dansrörelserna i min studie exemplifieras huvudsakligen av två korta utdrag ur Merce Cunninghams verk BIPED, representerade som subjektiva upplevelser av liveframträdanden, videoinspelningar, textrepresentationer och minnen. Dessutom överväger jag en textstudie om koreografiska praktiker av Susan Foster, och element från William Forsythes samling av koreografiska verktyg för rörelsegenerering.',\n",
       " 'Min definition av performativitet är baserad på traditionen för vanlig språkfilosofi av John L. Austin, som först introducerade idén om det performativa 1955, och den efterföljande kritiken av hans begrepp om performativitet av Jacques Derrida. De mer specifika performativa särdragen i min diskussion är dock närmare knutna till området genusvetenskap, särskilt Judith Butlers läsning av Derridas performativitet, och Susan Fosters kritik av Butler som alltför mycket oroad över verbala praktiker, i motsats till förkroppsligade, icke verbala praxis.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/app/data/stjernholm-texts/phd_thesis_sv.json\", 'r') as f:\n",
    "    translation_phd = json.load(f)\n",
    "\n",
    "print(type(translation_phd))\n",
    "print(len(translation_phd))\n",
    "translation_phd[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdddf899-c7cd-46f1-bc1f-257e318af766",
   "metadata": {},
   "source": [
    "### Load the Databricks Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a1062c-0c05-4cff-a987-2f3a0a64cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "with open(\"/app/data/databricks/databricks-dolly-15k.jsonl\") as file:\n",
    "    for line in file:\n",
    "        feature = json.loads(line)\n",
    "        \n",
    "        if feature[\"context\"]:\n",
    "            continue\n",
    "        \n",
    "        data.append(feature)\n",
    "\n",
    "data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97453ea8-1c43-4e84-9ec7-363521613301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset\n",
    "filtered_data = [\n",
    "    x[\"instruction\"]\n",
    "    + \" \"\n",
    "    + x[\"response\"]\n",
    "    for x in data\n",
    "    if x[\"instruction\"] is not None\n",
    "    and x[\"response\"] is not None\n",
    "]\n",
    "\n",
    "print(len(filtered_data))\n",
    "filtered_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30832e6c-485f-43c1-9b5d-43e095705d75",
   "metadata": {},
   "source": [
    "### Translate Databricks dataset to Swedish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1da016-48a5-4d54-98d9-720d70c02554",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Automated workflow for translating Databricks dataset to Swedish\n",
    "\n",
    "# Instantiate the Google Translator\n",
    "translator = GoogleTranslator(source='en', target='sv')\n",
    "\n",
    "# Create a List to store the translations in\n",
    "bricks_sv = []\n",
    "\n",
    "counter = 0\n",
    "for section in filtered_data:\n",
    "    # Clean the text\n",
    "    section = clean_text(section)\n",
    "    try:\n",
    "        # Show the progress for every 1000 section\n",
    "        if counter % 1000 == 0:\n",
    "            print(f\"Translated {counter} sections\")\n",
    "        counter += 1\n",
    "        # Send a batch to the translator and append to the above list\n",
    "        bricks_sv.append(translator.translate(section))\n",
    "    except:\n",
    "        print(\"Error: Could not translate a section.\")\n",
    "        \n",
    "# Save as a json file\n",
    "with open(\"/app/data/databricks/databricks-dolly-15k-sv.json\", \"w\") as f:\n",
    "    json.dump(bricks_sv, f)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184547a7-06ff-4a99-8b39-cbbda7ad6263",
   "metadata": {},
   "source": [
    "### Load Swedish Databricks dataset from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "327951e9-4d38-400b-b586-739fd3a75f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/app/data/databricks/databricks-dolly-15k-sv.json\", 'r') as f:\n",
    "    translation_bricks = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adff1754-4511-4eb1-b3ba-3a7ff53b48a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the dataset\n",
    "print(type(translation_bricks))\n",
    "print(len(translation_bricks))\n",
    "translation_bricks[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae2f5f4-d9fc-44aa-aecc-c05a507c6302",
   "metadata": {},
   "source": [
    "### Concatenate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77c28a08-5e9f-4c77-a4b2-f5da81915660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19001"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_data = translations_sv + translation_phd + translation_bricks\n",
    "len(complete_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe1572-f4f6-46a6-8bb2-fdbbc7165be2",
   "metadata": {},
   "source": [
    "## Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c83d0357-7def-4f52-85b7-df4eb363a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the punctuation, to treat them as separate 'words'\n",
    "def pad_punctuation(s):\n",
    "    s = re.sub(f\"([{string.punctuation}, '\\n'])\", r\" \\1 \", s)\n",
    "    s = re.sub(\" +\", \" \", s)\n",
    "    return s\n",
    "\n",
    "text_data = [pad_punctuation(x) for x in complete_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89850727-5608-4622-961e-7c46e9a30fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an example of a recipe\n",
    "example_data = text_data[25]\n",
    "example_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f38b5321-379f-4abe-8812-63ca97b11238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-13 06:24:05.199221: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-13 06:24:05.204463: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-13 06:24:05.204492: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-13 06:24:05.206999: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-13 06:24:05.207029: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-13 06:24:05.207040: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-13 06:24:05.437953: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-13 06:24:05.437992: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-13 06:24:05.437999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-04-13 06:24:05.438018: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-04-13 06:24:05.438043: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5663 MB memory:  -> device: 0, name: NVIDIA RTX A2000 8GB Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Convert to a Tensorflow Dataset\n",
    "text_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices(text_data)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .shuffle(1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "908b0ada-4ade-4e04-99b4-fc77bf19f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vectorisation layer\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=\"lower\",\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=MAX_LEN + 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "88a3f0b3-3ffa-4351-9c40-21e02e7ac183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-13 06:24:23.110105: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "# Adapt the layer to the training set\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b83ea-5efa-4603-a4f4-23105b4f0b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some token:word mappings\n",
    "for i, word in enumerate(vocab[:10]):\n",
    "    print(f\"{i}: {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dcb9fd-7a27-4d11-812e-0d5c3c7e3de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the same example converted to ints\n",
    "example_tokenised = vectorize_layer(example_data)\n",
    "print(example_tokenised.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95833ed6-0193-4a04-9edd-f05efed316a1",
   "metadata": {},
   "source": [
    "## Create the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d21a501-f5ec-4cd8-87b1-a0625e479429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training set of recipes and the same text shifted by one word\n",
    "def prepare_inputs(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# train_ds = text_ds.map(prepare_inputs)\n",
    "train_ds = text_ds.map(prepare_inputs).repeat(DATASET_REPETITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec8392-1daf-4ded-9169-6c57ccd4e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_output = train_ds.take(1).get_single_element()\n",
    "# Example Input\n",
    "example_input_output[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8364903d-12c0-4b45-9496-3015748f3601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Output (shifted by one token)\n",
    "example_input_output[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd31aef0-9c23-4f09-9337-087b421c5223",
   "metadata": {},
   "source": [
    "## Create the Causal Attention Mask function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "492308d1-a864-401c-92b7-3a008f5e3817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "np.transpose(causal_attention_mask(1, 10, 10, dtype=tf.int32)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b2e098-9362-4aa2-862d-e19c85e04ecb",
   "metadata": {},
   "source": [
    "## Create a Transformer Block layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ea66107-52d3-48d3-aee0-987c08841c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class TransformerBlock(layers.Layer):\n",
    "    # def __init__(self, num_heads, key_dim, embed_dim, ff_dim, dropout_rate=0.1):\n",
    "    def __init__(self, num_heads, key_dim, embed_dim, ff_dim, dropout_rate=0.1, **kwargs):\n",
    "        # super(TransformerBlock, self).__init__(**kwargs)\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout_rate = 0.1 # dropout_rate\n",
    "        self.attn = layers.MultiHeadAttention(\n",
    "            num_heads, key_dim, output_shape=embed_dim\n",
    "        )\n",
    "        self.dropout_1 = layers.Dropout(self.dropout_rate)\n",
    "        self.ln_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ffn_1 = layers.Dense(self.ff_dim, activation=\"relu\")\n",
    "        self.ffn_2 = layers.Dense(self.embed_dim)\n",
    "        self.dropout_2 = layers.Dropout(self.dropout_rate)\n",
    "        self.ln_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(\n",
    "            batch_size, seq_len, seq_len, tf.bool\n",
    "        )\n",
    "        attention_output, attention_scores = self.attn(\n",
    "            inputs,\n",
    "            inputs,\n",
    "            attention_mask=causal_mask,\n",
    "            return_attention_scores=True,\n",
    "        )\n",
    "        attention_output = self.dropout_1(attention_output)\n",
    "        out1 = self.ln_1(inputs + attention_output)\n",
    "        ffn_1 = self.ffn_1(out1)\n",
    "        ffn_2 = self.ffn_2(ffn_1)\n",
    "        ffn_output = self.dropout_2(ffn_2)\n",
    "        return (self.ln_2(out1 + ffn_output), attention_scores)\n",
    "\n",
    "    # def get_config(self):\n",
    "    #     config = super().get_config()\n",
    "    #     config.update(\n",
    "    #         {\n",
    "    #             \"key_dim\": self.key_dim,\n",
    "    #             \"embed_dim\": self.embed_dim,\n",
    "    #             \"num_heads\": self.num_heads,\n",
    "    #             \"ff_dim\": self.ff_dim,\n",
    "    #             \"dropout_rate\": self.dropout_rate,\n",
    "    #         }\n",
    "    #     )\n",
    "    #     return config\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"key_dim\": keras.saving.serialize_keras_object(self.key_dim),\n",
    "                \"embed_dim\": keras.saving.serialize_keras_object(self.embed_dim),\n",
    "                \"num_heads\": keras.saving.serialize_keras_object(self.num_heads),\n",
    "                \"ff_dim\": keras.saving.serialize_keras_object(self.ff_dim),\n",
    "                \"dropout_rate\": keras.saving.serialize_keras_object(self.dropout_rate),\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        key_dim_config = config[\"key_dim\"]\n",
    "        embed_dim_config = config[\"embed_dim\"]\n",
    "        num_heads_config = config[\"num_heads\"]\n",
    "        ff_dim_config = config[\"ff_dim\"]\n",
    "        dropout_rate_config = config[\"dropout_rate\"]\n",
    "        key_dim = keras.saving.deserialize_keras_object(key_dim_config)\n",
    "        embed_dim = keras.saving.deserialize_keras_object(embed_dim_config)\n",
    "        num_heads = keras.saving.deserialize_keras_object(num_heads_config)\n",
    "        ff_dim = keras.saving.deserialize_keras_object(ff_dim_config)\n",
    "        num_heads = keras.saving.deserialize_keras_object(num_heads_config)\n",
    "        return cls(key_dim, embed_dim, num_heads, ff_dim, num_heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc16c72-29c4-4e32-b68e-353d414d0d81",
   "metadata": {},
   "source": [
    "## Create Token and Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "021887e2-9049-4530-98e9-f3de4ab4b7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    # def __init__(self, max_len, vocab_size, embed_dim):\n",
    "    def __init__(self, max_len, vocab_size, embed_dim, **kwargs):\n",
    "        # super(TokenAndPositionEmbedding, self).__init__()\n",
    "        super(TokenAndPositionEmbedding, self).__init__(**kwargs)\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.token_emb = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.pos_emb = layers.Embedding(input_dim=max_len, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "    # def get_config(self):\n",
    "    #     config = super().get_config()\n",
    "    #     config.update(\n",
    "    #         {\n",
    "    #             \"max_len\": self.max_len,\n",
    "    #             \"vocab_size\": self.vocab_size,\n",
    "    #             \"embed_dim\": self.embed_dim,\n",
    "    #         }\n",
    "    #     )\n",
    "    #     return config\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "            \"max_len\": keras.saving.serialize_keras_object(self.max_len),\n",
    "            \"vocab_size\": keras.saving.serialize_keras_object(self.vocab_size),\n",
    "            \"embed_dim\": keras.saving.serialize_keras_object(self.embed_dim),\n",
    "            }   \n",
    "        )  \n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # sublayer_config = config.pop(\"sublayer\")\n",
    "        max_len_config = config[\"max_len\"]\n",
    "        vocab_size_config = config[\"vocab_size\"]\n",
    "        embed_dim_config = config[\"embed_dim\"]\n",
    "        max_len = keras.saving.deserialize_keras_object(max_len_config)\n",
    "        vocab_size = keras.saving.deserialize_keras_object(vocab_size_config)\n",
    "        embed_dim = keras.saving.deserialize_keras_object(embed_dim_config)\n",
    "        return cls(max_len, vocab_size, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db7f2dd-0cca-469b-947f-717e0b57514e",
   "metadata": {},
   "source": [
    "## Build the Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44e0f453-1bee-4761-8b92-dee8f7a423a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
    "x = TokenAndPositionEmbedding(MAX_LEN, VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
    "x, attention_scores = TransformerBlock(\n",
    "    N_HEADS, KEY_DIM, EMBEDDING_DIM, FEED_FORWARD_DIM\n",
    ")(x)\n",
    "outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "gpt = models.Model(inputs=inputs, outputs=[outputs, attention_scores])\n",
    "gpt.compile(\"adam\", loss=[losses.SparseCategoricalCrossentropy(), None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c53d0cb0-e0af-47ed-8524-240b12d4b12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ token_and_position_embedding_4  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">51,240,960</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_4             │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>),    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,728,320</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)] │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100000</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">51,300,000</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ token_and_position_embedding_4  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │    \u001b[38;5;34m51,240,960\u001b[0m │\n",
       "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block_4             │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m),    │     \u001b[38;5;34m4,728,320\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)] │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100000\u001b[0m)   │    \u001b[38;5;34m51,300,000\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">107,269,280</span> (409.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m107,269,280\u001b[0m (409.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">107,269,280</span> (409.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m107,269,280\u001b[0m (409.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c8d928b0-a0c3-4c68-86f7-3a0472e0a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if LOAD_MODEL:\n",
    "if True:\n",
    "    gpt.load_weights(\"./checkpoint/checkpoint.weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d911e71-4825-4e49-b563-8fe860f42d67",
   "metadata": {},
   "source": [
    "## Train the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7900628a-2f20-43ac-ab41-7079f0ab88e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TextGenerator checkpoint\n",
    "class TextGenerator(callbacks.Callback):\n",
    "    def __init__(self, index_to_word, top_k=10):\n",
    "        self.index_to_word = index_to_word\n",
    "        self.word_to_index = {\n",
    "            word: index for index, word in enumerate(index_to_word)\n",
    "        }\n",
    "\n",
    "    def sample_from(self, probs, temperature):\n",
    "        probs = probs ** (1 / temperature)\n",
    "        probs = probs / np.sum(probs)\n",
    "        return np.random.choice(len(probs), p=probs), probs\n",
    "\n",
    "    def generate(self, start_prompt, max_tokens, temperature):\n",
    "        start_tokens = [\n",
    "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
    "        ]\n",
    "        sample_token = None\n",
    "        info = []\n",
    "        while len(start_tokens) < max_tokens and sample_token != 0:\n",
    "            x = np.array([start_tokens])\n",
    "            y, att = self.model.predict(x, verbose=0)\n",
    "            sample_token, probs = self.sample_from(y[0][-1], temperature)\n",
    "            info.append(\n",
    "                {\n",
    "                    \"prompt\": start_prompt,\n",
    "                    \"word_probs\": probs,\n",
    "                    \"atts\": att[0, :, -1, :],\n",
    "                }\n",
    "            )\n",
    "            start_tokens.append(sample_token)\n",
    "            start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
    "        print(f\"\\ngenerated text:\\n{start_prompt}\\n\")\n",
    "        return info\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.generate(\"Vatten\", max_tokens=80, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "60a93904-cdcc-465a-98da-533cf8ec3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model save checkpoint\n",
    "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=\"./checkpoint/checkpoint.weights.h5\",\n",
    "    save_weights_only=True,\n",
    "    save_freq=\"epoch\",\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "tensorboard_callback = callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "\n",
    "# Tokenize starting prompt\n",
    "text_generator = TextGenerator(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cbf863ea-c126-4f8c-b44c-3d0d9299aaa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1712990210.568459    7169 service.cc:145] XLA service 0x7f1424012b50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1712990210.568513    7169 service.cc:153]   StreamExecutor device (0): NVIDIA RTX A2000 8GB Laptop GPU, Compute Capability 8.6\n",
      "2024-04-13 06:36:50.615815: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "W0000 00:00:1712990210.704922    7169 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2024-04-13 06:36:50.842635: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8906\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1712990212.811731    7415 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 304 bytes spill stores, 304 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990212.901557    7426 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_7', 456 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990212.908993    7417 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_7', 172 bytes spill stores, 172 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990213.008781    7422 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 260 bytes spill stores, 260 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990213.289481    7427 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 672 bytes spill stores, 640 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990213.441877    7423 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990213.856649    7425 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990213.922745    7426 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 644 bytes spill stores, 612 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990214.176346    7411 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_32', 204 bytes spill stores, 160 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990214.281120    7417 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_17', 156 bytes spill stores, 120 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990215.376315    7426 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_17', 404 bytes spill stores, 376 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990215.584905    7424 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_42', 192 bytes spill stores, 160 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990215.852103    7418 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_41', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990216.029527    7419 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 644 bytes spill stores, 612 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990216.099413    7416 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 260 bytes spill stores, 260 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990216.198394    7413 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990216.742821    7417 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_15', 84 bytes spill stores, 84 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990216.874900    7409 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_32', 184 bytes spill stores, 184 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990217.390239    7418 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_38', 212 bytes spill stores, 116 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990217.584699    7415 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_38', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990231.761907    7169 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m136/594\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 167ms/step - loss: 0.4016"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1712990254.852151    7171 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "I0000 00:00:1712990256.438631    7729 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990256.439348    7735 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 48 bytes spill stores, 48 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990256.595495    7733 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 316 bytes spill stores, 316 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990256.695152    7740 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 712 bytes spill stores, 712 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990256.963213    7729 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 92 bytes spill stores, 52 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990257.026105    7726 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990257.206992    7740 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990257.339712    7723 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_17', 440 bytes spill stores, 436 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990257.751910    7734 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 660 bytes spill stores, 964 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990258.172679    7724 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990258.660983    7741 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 660 bytes spill stores, 372 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990258.975099    7727 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_38', 212 bytes spill stores, 116 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990259.090865    7724 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_32', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990259.138460    7729 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 660 bytes spill stores, 372 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990259.374457    7735 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_32', 204 bytes spill stores, 204 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990259.416429    7734 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_7', 468 bytes spill stores, 276 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990259.511845    7737 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_7', 28 bytes spill stores, 20 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990259.557445    7736 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_38', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990259.953416    7738 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_33', 408 bytes spill stores, 404 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990260.452518    7739 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 660 bytes spill stores, 964 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990261.368209    7737 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_33', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m594/594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 199ms/step - loss: 0.4327"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1712990352.105857    8032 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990352.786423    8030 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 328 bytes spill stores, 328 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990354.448394    8141 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990354.738891    8135 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 332 bytes spill stores, 332 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990355.903508    8254 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990356.676164    8252 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 328 bytes spill stores, 328 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990357.664972    8360 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990358.382486    8351 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 332 bytes spill stores, 332 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990360.003353    8456 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 328 bytes spill stores, 328 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990360.026904    8470 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990361.124016    8571 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990361.717496    8582 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 332 bytes spill stores, 332 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990363.201118    8682 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990363.492317    8686 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 328 bytes spill stores, 328 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990364.608397    8788 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990365.151707    8780 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 336 bytes spill stores, 336 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990365.154813    8787 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990366.228912    8893 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990366.679284    8904 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 328 bytes spill stores, 328 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990366.699583    8889 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990366.752733    8897 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990368.368271    9008 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990368.370513    9010 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 336 bytes spill stores, 336 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990368.573364    8996 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990369.499443    9117 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990369.618141    9121 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990369.727656    9111 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990370.247657    9111 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 328 bytes spill stores, 328 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990371.569892    9228 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990371.671962    9222 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990371.684674    9230 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 336 bytes spill stores, 336 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990373.355287    9331 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990373.452772    9339 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990373.569121    9331 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990373.876420    9335 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 328 bytes spill stores, 328 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990375.229670    9430 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990375.261116    9446 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 24 bytes spill stores, 24 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990375.644594    9429 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 336 bytes spill stores, 336 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990376.692868    9539 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990376.855858    9552 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 8 bytes spill stores, 8 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990377.114355    9546 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 332 bytes spill stores, 332 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990378.362941    9650 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990378.387611    9660 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990378.912543    9654 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 336 bytes spill stores, 336 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990379.304348    9657 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 400 bytes spill stores, 400 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990380.845560    9779 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 328 bytes spill stores, 328 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990380.885825    9766 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990381.044883    9775 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 408 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990382.809208    9891 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990383.090417    9896 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990383.718296    9890 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 400 bytes spill stores, 400 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990383.889504    9880 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 336 bytes spill stores, 336 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990385.244619   10005 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990385.928354    9996 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 328 bytes spill stores, 328 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990386.366372   10002 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 388 bytes spill stores, 388 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990387.982114   10122 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990388.302997   10127 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 336 bytes spill stores, 336 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990388.497801   10118 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 408 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990388.606234   10114 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990390.103136   10234 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990390.809194   10229 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 328 bytes spill stores, 328 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990391.071279   10234 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 408 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990392.338995   10343 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990392.706815   10360 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990393.019481   10354 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 336 bytes spill stores, 336 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990393.108748   10362 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 408 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990394.315659   10478 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990395.261736   10470 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 388 bytes spill stores, 388 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990395.369983   10463 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 328 bytes spill stores, 328 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990397.140955   10595 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990397.158901   10586 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990397.571691   10593 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 336 bytes spill stores, 336 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990397.805546   10591 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 404 bytes spill stores, 404 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990399.862291   10704 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 328 bytes spill stores, 328 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990400.015440   10705 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 408 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990400.213683   10695 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990401.959579   10817 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990402.545012   10808 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990402.807931   10827 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990402.846534   10817 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 412 bytes spill stores, 412 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990402.875844   10819 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 336 bytes spill stores, 336 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990404.642148   10942 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 12 bytes spill stores, 12 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990404.761702   10937 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990404.811696   10928 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 328 bytes spill stores, 328 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990405.068280   10934 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 388 bytes spill stores, 388 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990406.522157   11057 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990406.833777   11055 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 336 bytes spill stores, 336 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990406.901663   11046 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990407.005159   11052 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 404 bytes spill stores, 404 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990409.069266   11175 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990409.120249   11168 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_7', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990409.720744   11160 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 408 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990409.788056   11162 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 328 bytes spill stores, 328 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990411.572535   11289 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990411.744000   11288 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 4 bytes spill stores, 4 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990411.927968   11284 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990412.030944   11292 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 336 bytes spill stores, 336 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990412.048787   11275 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 404 bytes spill stores, 404 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990414.109754   11397 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990414.646451   11398 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 324 bytes spill stores, 324 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990414.667044   11396 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 332 bytes spill stores, 332 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990416.366929   11521 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 140 bytes spill stores, 140 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990416.481810   11515 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 336 bytes spill stores, 336 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990416.524422   11520 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990416.673492   11513 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 412 bytes spill stores, 412 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990420.435335   11648 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990420.718193   11645 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990420.850077   11647 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 328 bytes spill stores, 328 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990421.003001   11642 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 408 bytes spill stores, 408 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990423.317530   11770 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 36 bytes spill stores, 36 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990423.435527   11774 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990423.687207   11768 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 412 bytes spill stores, 412 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990424.199289   11782 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 140 bytes spill stores, 140 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990424.318339   11778 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 336 bytes spill stores, 336 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990425.632253   11909 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990426.391641   11913 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 388 bytes spill stores, 388 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990426.511717   11895 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 328 bytes spill stores, 328 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990426.689391   11907 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990428.944912   12028 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990429.262822   12034 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_10', 28 bytes spill stores, 28 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990429.673364   12042 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 140 bytes spill stores, 140 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990430.101999   12032 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 336 bytes spill stores, 336 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990430.175012   12025 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 412 bytes spill stores, 412 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990432.035210   12159 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 44 bytes spill stores, 44 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990432.151937   12154 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_12', 16 bytes spill stores, 16 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990432.215035   12168 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 328 bytes spill stores, 328 bytes spill loads\n",
      "\n",
      "I0000 00:00:1712990432.342953   12152 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_9', 408 bytes spill stores, 408 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "generated text:\n",
      "Vatten är inte skattedagen den 15 april ? inkomstskatter i allmänhet måste i allmänhet lämnas in senast den 15 april . men när den 15 april infaller på en helg eller helgdag flyttas anmälningstiden till nästa arbetsdag . \n",
      "\n",
      "\u001b[1m594/594\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 341ms/step - loss: 0.4327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f14e5101e90>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[model_checkpoint_callback, tensorboard_callback, text_generator],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2483ae6-a4d6-442f-a434-5e6ef054141d",
   "metadata": {},
   "source": [
    "### Save the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50750062-ecf8-4652-80d5-f0bb49b2c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "gpt.save(\"./models/gpt.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10594ae1-1edf-481d-8fe3-45e80f9db10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:359: UserWarning: `build()` was called on layer 'token_and_position_embedding_6', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/layer.py:359: UserWarning: `build()` was called on layer 'transformer_block_6', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Exception encountered when calling TokenAndPositionEmbedding.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'token_and_position_embedding_6' (of type TokenAndPositionEmbedding). Either the `TokenAndPositionEmbedding.call()` method is incorrect, or you need to implement the `TokenAndPositionEmbedding.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nVariable transformer_block_5/multi_head_attention_5/query/kernel is already initialized.\u001b[0m\n\nArguments received by TokenAndPositionEmbedding.call():\n  • args=('<KerasTensor shape=(None, None), dtype=int32, sparse=False, name=input_layer_4>',)\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m new_model \u001b[38;5;241m=\u001b[39m \u001b[43msaving\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./models/gpt.keras\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTokenAndPositionEmbedding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mTokenAndPositionEmbedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransformerBlock\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mTransformerBlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_api.py:176\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    173\u001b[0m         is_keras_zip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_keras_zip:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(filepath)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:155\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ObjectSharingScope():\n\u001b[0;32m--> 155\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msafe_mode\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m all_filenames \u001b[38;5;241m=\u001b[39m zf\u001b[38;5;241m.\u001b[39mnamelist()\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _VARS_FNAME \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m all_filenames:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/saving/serialization_lib.py:711\u001b[0m, in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(config, custom_objects, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m custom_obj_scope, safe_mode_scope:\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 711\u001b[0m         instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43minner_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    713\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    714\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m could not be deserialized properly. Please\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    715\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m ensure that components that are Python object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mconfig=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mException encountered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    720\u001b[0m         )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/model.py:492\u001b[0m, in \u001b[0;36mModel.from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_functional_config \u001b[38;5;129;01mand\u001b[39;00m revivable_as_functional:\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;66;03m# Revive Functional model\u001b[39;00m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;66;03m# (but not Functional subclasses with a custom __init__)\u001b[39;00m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional_from_config\n\u001b[0;32m--> 492\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunctional_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# Either the model has a custom __init__, or the config\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# does not contain all the information necessary to\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;66;03m# revive a Functional model. This happens when the user creates\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;66;03m# In this case, we fall back to provide all config into the\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;66;03m# constructor of the class.\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:522\u001b[0m, in \u001b[0;36mfunctional_from_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    520\u001b[0m node_data \u001b[38;5;241m=\u001b[39m node_data_list[node_index]\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 522\u001b[0m     \u001b[43mprocess_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;66;03m# If the node does not have all inbound layers\u001b[39;00m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;66;03m# available, stop processing and continue later\u001b[39;00m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/models/functional.py:469\u001b[0m, in \u001b[0;36mfunctional_from_config.<locals>.process_node\u001b[0;34m(layer, node_data)\u001b[0m\n\u001b[1;32m    466\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m deserialize_node(node_data, created_layers)\n\u001b[1;32m    467\u001b[0m \u001b[38;5;66;03m# Call layer on its inputs, thus creating the node\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;66;03m# and building the layer if needed.\u001b[39;00m\n\u001b[0;32m--> 469\u001b[0m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/core.py:37\u001b[0m, in \u001b[0;36mVariable._deferred_initialize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_deferred_initialize\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 37\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVariable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is already initialized.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m in_stateless_scope():\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     41\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are attempting to initialize a variable \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile in a stateless scope. This is disallowed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     43\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that all variables are initialized \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbefore you start using your layer/model objects.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception encountered when calling TokenAndPositionEmbedding.call().\n\n\u001b[1mCould not automatically infer the output shape / dtype of 'token_and_position_embedding_6' (of type TokenAndPositionEmbedding). Either the `TokenAndPositionEmbedding.call()` method is incorrect, or you need to implement the `TokenAndPositionEmbedding.compute_output_spec() / compute_output_shape()` method. Error encountered:\n\nVariable transformer_block_5/multi_head_attention_5/query/kernel is already initialized.\u001b[0m\n\nArguments received by TokenAndPositionEmbedding.call():\n  • args=('<KerasTensor shape=(None, None), dtype=int32, sparse=False, name=input_layer_4>',)\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "new_model = saving.load_model(\n",
    "    \"./models/gpt.keras\",\n",
    "    custom_objects={\n",
    "        'TokenAndPositionEmbedding': TokenAndPositionEmbedding,\n",
    "        'TransformerBlock': TransformerBlock,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461afe63-126a-422d-9337-4446f936f0a1",
   "metadata": {},
   "source": [
    "# Text to Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739dfbf5-fe56-4c86-b190-fd5b166defd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tts = gTTS('Jag heter Johan', lang='sv', slow=True)\n",
    "tts.save('hello.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd744e51-afc7-43e6-ae81-f2adf092a15d",
   "metadata": {},
   "source": [
    "# Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f3b398d-d22f-4edf-8fed-be3ef83bb755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "generated text:\n",
      "Meningen med livet och det finns inget sätt att bo i val , men det finns många sätt att bo i en dröm . en bra person kan ses som ett nytt hus . den här listan är några förslag på att tala i byggde den dåliga stora pyramiden för av kanalen i den här världen : * höjd kryper här är några enkla stora snacks , utmärkta kollektivtrafik , * kollektivtrafik , kollektivtrafiken eller transporter . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "info = text_generator.generate(\n",
    "    \"Meningen med livet\", max_tokens=80, temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328912c5-0a88-4e24-83ef-671a70c2b43a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
