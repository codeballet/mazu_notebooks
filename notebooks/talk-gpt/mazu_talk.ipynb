{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d5f9aab-8560-4434-98a9-f8327f22b8d0",
   "metadata": {},
   "source": [
    "# Mazu Talk\n",
    "Mazu Talk is a GPT style, Transformer based Decoder. The code is adapted from two sources:\n",
    "* the [GPT tutorial](https://keras.io/examples/generative/text_generation_with_miniature_gpt/) by Apoorv Nandan available on the Keras website.\n",
    "* Generative Deep Learning, 2nd edition, by David Foster (O’Reilly), 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befa48ea-8500-48c6-be1a-47c374183979",
   "metadata": {},
   "source": [
    "## Install libraries and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4aa511d-3526-4dbd-aa47-5dec53f83643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deep-translator in /usr/local/lib/python3.11/dist-packages (1.11.4)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from deep-translator) (2.31.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2024.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m/usr/bin/sh: 1: poetry: not found\n",
      "Requirement already satisfied: gTTS in /usr/local/lib/python3.11/dist-packages (2.5.1)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from gTTS) (2.31.0)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in /usr/local/lib/python3.11/dist-packages (from gTTS) (8.1.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->gTTS) (2024.2.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U deep-translator\n",
    "!poetry add deep-translator   # for poetry usage\n",
    "!pip install gTTS\n",
    "\n",
    "from deep_translator import GoogleTranslator\n",
    "from gtts import gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f479a8dd-2828-436b-a365-63ee8a3d5e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 12:26:19.654948: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import glob\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from IPython.display import display, HTML\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, losses, callbacks, saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8cfd20b-1277-48cf-8f78-2eabd62abb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set min. log level for TF to mute warnings\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f53356-c960-4be6-830a-ee35ffcb24dc",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2080df5-113d-48ca-888c-3dee16292197",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 100000\n",
    "MAX_LEN = 80\n",
    "# EMBEDDING_DIM = 256\n",
    "EMBEDDING_DIM = 512\n",
    "# KEY_DIM = 256\n",
    "KEY_DIM = 512\n",
    "N_HEADS = 4\n",
    "# FEED_FORWARD_DIM = 256\n",
    "FEED_FORWARD_DIM = 512\n",
    "VALIDATION_SPLIT = 0.2\n",
    "SEED = 42\n",
    "LOAD_MODEL = False\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "DATASET_REPETITIONS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f1c42f-35a4-4f68-9481-a02d1fa96b76",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "Chinese Poems are sourced from:\n",
    "* https://www.kaggle.com/datasets/qianboao/chinesepoetrydataset\n",
    "* https://github.com/chinese-poetry/chinese-poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fc730e-80c1-4812-832e-1da30e668f2c",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61957c4d-110b-4521-86c6-62ee1a4767b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean a text from special characters\n",
    "def clean_text(text):\n",
    "    # Remove content within brackets\n",
    "    pattern_brackets = r'\\(.*?\\)'\n",
    "    cleaned_text = re.sub(pattern_brackets, '', text)\n",
    "    \n",
    "    # Remove newline characters (\\n) and tab characters (\\t)\n",
    "    cleaned_text = cleaned_text.replace('\\n', '').replace('\\t', '')\n",
    "    \n",
    "    # Replace hyphens with whitespace\n",
    "    cleaned_text = cleaned_text.replace('-', ' ')\n",
    "    \n",
    "    # Remove curly double quotes\n",
    "    cleaned_text = cleaned_text.replace(\"“\", '').replace(\"”\", '')\n",
    "\n",
    "    \n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbf0b4d-0e11-484c-b3f2-6c9e2c7c2019",
   "metadata": {},
   "source": [
    "### Load Chinese poems from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b8f5b62-1319-4622-816e-f9920c0c537d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['欲出未出光辣达,千山万山如火发.须臾走向天上来,逐却残星赶却月.\\n', '满目江山四望幽,白云高卷嶂烟收.日回禽影穿疏木,风递猿声入小楼.远岫似屏横碧落,断帆如叶截中流.\\n', '片片飞来静又闲,楼头江上复山前.飘零尽日不归去,帖破清光万里天.\\n', '因登巨石知来处,勃勃元生绿藓痕.静即等闲藏草木,动时顷刻徧乾坤.横天未必朋元恶,捧日还曾瑞至尊.不独朝朝在巫峡,楚王何事谩劳魂.\\n', '一气东南王斗牛,祖龙潜为子孙忧.金陵地脉何曾断,不觉真人已姓刘.\\n']\n",
      "304752\n"
     ]
    }
   ],
   "source": [
    "# Open file containing Chinese poetry\n",
    "with open('/app/data/chinese-poetry/chinese_poems.txt', 'r') as f:\n",
    "    zh_poems = f.readlines()\n",
    "    \n",
    "print(zh_poems[:5])\n",
    "print(len(zh_poems))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d71e16e-1e42-4d06-8ccd-4850a8c415e7",
   "metadata": {},
   "source": [
    "### Translate Chinese poems to Swedish and save as files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "829c1a70-ccc7-4cb9-9fb4-ea518fbffcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translating batch 7000\n",
      "0 translations done!\n",
      "200 translations done!\n",
      "400 translations done!\n",
      "600 translations done!\n",
      "800 translations done!\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Automated workflow for translating Chinese poems\n",
    "\n",
    "# Instantiate the Google Translator\n",
    "translator = GoogleTranslator(source='zh-CN', target='sv')\n",
    "\n",
    "for i in range(7000, 8000, 1000):\n",
    "    print(f\"translating batch {i}\")\n",
    "    # Create a List to store the translations in\n",
    "    zh_poems_sv = []\n",
    "    counter = 0\n",
    "\n",
    "    for poem in zh_poems[i: i + 1000]:\n",
    "        # Inform progress\n",
    "        if counter % 200 == 0:\n",
    "            print(f\"{counter} translations done!\")\n",
    "        counter += 1\n",
    "        # Clean the text\n",
    "        poem = clean_text(poem)\n",
    "        try:\n",
    "            # Send a batch to the translator and append to the above list\n",
    "            zh_poems_sv.append(translator.translate(poem))\n",
    "        except:\n",
    "            print(\"Error: Could not translate a poem.\")\n",
    "    # Save the batch as a json file\n",
    "    with open(\"/app/data/zh_poems_sv/zh_poems_sv_%000006d_%000006d.json\" % (i, i + 1000), 'w') as f:\n",
    "        json.dump(zh_poems_sv, f)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f5b5dd-17ca-42c7-8a29-ef67c506c549",
   "metadata": {},
   "source": [
    "### Load Swedish translations of Chinese poems from saved files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a1101f-3536-444e-b1e0-c1a8f6bb82f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the files\n",
    "file_list = glob.glob(\"/app/data/zh_poems_sv/*.json\")\n",
    "print(f\"Found {len(file_list)} files\")\n",
    "file_list\n",
    "\n",
    "# Put the file contents in a list\n",
    "translations_sv = []\n",
    "for file in file_list:\n",
    "    with open(file, 'r') as f:\n",
    "        for poem in json.load(f):\n",
    "            translations_sv.append(poem)\n",
    "\n",
    "# Print some examples of the list\n",
    "print(f\"Found {len(translations_sv)} poems\")\n",
    "translations_sv[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06512e7b-4958-4bf4-9294-487aa6a190ee",
   "metadata": {},
   "source": [
    "### Load PhD, English version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccef935-3223-4f2c-a383-8c86e2a61fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/app/data/stjernholm-texts/phd_thesis.txt', 'r') as f:\n",
    "    phd = f.readlines()\n",
    "phd[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d280e8-068b-40dc-9df7-289a4cc40df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove headings and short paragraphs\n",
    "def filter_long_strings(input_list, min_length=40):\n",
    "    \"\"\"\n",
    "    Removes strings from the input list that have a length less than min_length.\n",
    "    Args:\n",
    "        input_list (list): List of strings.\n",
    "        min_length (int, optional): Minimum length for strings to keep. Defaults to 40.\n",
    "\n",
    "    Returns:\n",
    "        list: Filtered list containing only strings with length greater than or equal to min_length.\n",
    "    \"\"\"\n",
    "    return [string for string in input_list if len(string) > min_length]\n",
    "\n",
    "# Example usage:\n",
    "shorter_phd = filter_long_strings(phd)\n",
    "shorter_phd[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6793b7d1-a404-405b-8d21-3707bf669759",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_phd = [clean_text(x) for x in shorter_phd]\n",
    "cleaned_phd[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0860b9-ffb5-49d6-8779-e7d721af1422",
   "metadata": {},
   "source": [
    "### Translate PhD to Swedish and save as files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474faa28-125a-4fd5-b34c-2ad9702e45bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated workflow for translating PhD to Swedish\n",
    "\n",
    "# Instantiate the Google Translator\n",
    "translator = GoogleTranslator(source='en', target='sv')\n",
    "\n",
    "# Create a List to store the translations in\n",
    "phd_sv = []\n",
    "\n",
    "for section in cleaned_phd:\n",
    "    # Clean the text\n",
    "    section = clean_text(section)\n",
    "    try:\n",
    "        # Send a batch to the translator and append to the above list\n",
    "        phd_sv.append(translator.translate(section))\n",
    "    except:\n",
    "        print(\"Error: Could not translate a section.\")\n",
    "        \n",
    "# Save as a json file\n",
    "with open(\"/app/data/stjernholm-texts/phd_thesis_sv.json\", \"w\") as f:\n",
    "    json.dump(phd_sv, f)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c81d734-4118-4949-9b88-10c2af532a09",
   "metadata": {},
   "source": [
    "### Load PhD Swedish version from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8e2371-7226-467d-ab03-7a6c1b9e6a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/app/data/stjernholm-texts/phd_thesis_sv.json\", 'r') as f:\n",
    "    translation_phd = json.load(f)\n",
    "\n",
    "print(type(translation_phd))\n",
    "print(len(translation_phd))\n",
    "translation_phd[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdddf899-c7cd-46f1-bc1f-257e318af766",
   "metadata": {},
   "source": [
    "### Load the Databricks Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a1062c-0c05-4cff-a987-2f3a0a64cab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "with open(\"/app/data/databricks/databricks-dolly-15k.jsonl\") as file:\n",
    "    for line in file:\n",
    "        feature = json.loads(line)\n",
    "        \n",
    "        if feature[\"context\"]:\n",
    "            continue\n",
    "        \n",
    "        data.append(feature)\n",
    "\n",
    "data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97453ea8-1c43-4e84-9ec7-363521613301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset\n",
    "filtered_data = [\n",
    "    x[\"instruction\"]\n",
    "    + \" \"\n",
    "    + x[\"response\"]\n",
    "    for x in data\n",
    "    if x[\"instruction\"] is not None\n",
    "    and x[\"response\"] is not None\n",
    "]\n",
    "\n",
    "print(len(filtered_data))\n",
    "filtered_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30832e6c-485f-43c1-9b5d-43e095705d75",
   "metadata": {},
   "source": [
    "### Translate Databricks dataset to Swedish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1da016-48a5-4d54-98d9-720d70c02554",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Automated workflow for translating Databricks dataset to Swedish\n",
    "\n",
    "# Instantiate the Google Translator\n",
    "translator = GoogleTranslator(source='en', target='sv')\n",
    "\n",
    "# Create a List to store the translations in\n",
    "bricks_sv = []\n",
    "\n",
    "counter = 0\n",
    "for section in filtered_data:\n",
    "    # Clean the text\n",
    "    section = clean_text(section)\n",
    "    try:\n",
    "        # Show the progress for every 1000 section\n",
    "        if counter % 1000 == 0:\n",
    "            print(f\"Translated {counter} sections\")\n",
    "        counter += 1\n",
    "        # Send a batch to the translator and append to the above list\n",
    "        bricks_sv.append(translator.translate(section))\n",
    "    except:\n",
    "        print(\"Error: Could not translate a section.\")\n",
    "        \n",
    "# Save as a json file\n",
    "with open(\"/app/data/databricks/databricks-dolly-15k-sv.json\", \"w\") as f:\n",
    "    json.dump(bricks_sv, f)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184547a7-06ff-4a99-8b39-cbbda7ad6263",
   "metadata": {},
   "source": [
    "### Load Swedish Databricks dataset from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327951e9-4d38-400b-b586-739fd3a75f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/app/data/databricks/databricks-dolly-15k-sv.json\", 'r') as f:\n",
    "    translation_bricks = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adff1754-4511-4eb1-b3ba-3a7ff53b48a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the dataset\n",
    "print(type(translation_bricks))\n",
    "print(len(translation_bricks))\n",
    "translation_bricks[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae2f5f4-d9fc-44aa-aecc-c05a507c6302",
   "metadata": {},
   "source": [
    "### Concatenate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c28a08-5e9f-4c77-a4b2-f5da81915660",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data = translations_sv + translation_phd + translation_bricks\n",
    "len(complete_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fe1572-f4f6-46a6-8bb2-fdbbc7165be2",
   "metadata": {},
   "source": [
    "## Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83d0357-7def-4f52-85b7-df4eb363a690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the punctuation, to treat them as separate 'words'\n",
    "def pad_punctuation(s):\n",
    "    s = re.sub(f\"([{string.punctuation}, '\\n'])\", r\" \\1 \", s)\n",
    "    s = re.sub(\" +\", \" \", s)\n",
    "    return s\n",
    "\n",
    "text_data = [pad_punctuation(x) for x in complete_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89850727-5608-4622-961e-7c46e9a30fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an example of a recipe\n",
    "example_data = text_data[25]\n",
    "example_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38b5321-379f-4abe-8812-63ca97b11238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a Tensorflow Dataset\n",
    "text_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices(text_data)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .shuffle(1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908b0ada-4ade-4e04-99b4-fc77bf19f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vectorisation layer\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=\"lower\",\n",
    "    max_tokens=VOCAB_SIZE,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=MAX_LEN + 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a3f0b3-3ffa-4351-9c40-21e02e7ac183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt the layer to the training set\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b83ea-5efa-4603-a4f4-23105b4f0b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some token:word mappings\n",
    "for i, word in enumerate(vocab[:10]):\n",
    "    print(f\"{i}: {word}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dcb9fd-7a27-4d11-812e-0d5c3c7e3de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the same example converted to ints\n",
    "example_tokenised = vectorize_layer(example_data)\n",
    "print(example_tokenised.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95833ed6-0193-4a04-9edd-f05efed316a1",
   "metadata": {},
   "source": [
    "## Create the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d21a501-f5ec-4cd8-87b1-a0625e479429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training set of recipes and the same text shifted by one word\n",
    "def prepare_inputs(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "# train_ds = text_ds.map(prepare_inputs)\n",
    "train_ds = text_ds.map(prepare_inputs).repeat(DATASET_REPETITIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ec8392-1daf-4ded-9169-6c57ccd4e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_output = train_ds.take(1).get_single_element()\n",
    "# Example Input\n",
    "example_input_output[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8364903d-12c0-4b45-9496-3015748f3601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Output (shifted by one token)\n",
    "example_input_output[1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd31aef0-9c23-4f09-9337-087b421c5223",
   "metadata": {},
   "source": [
    "## Create the Causal Attention Mask function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492308d1-a864-401c-92b7-3a008f5e3817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "np.transpose(causal_attention_mask(1, 10, 10, dtype=tf.int32)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b2e098-9362-4aa2-862d-e19c85e04ecb",
   "metadata": {},
   "source": [
    "## Create a Transformer Block layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea66107-52d3-48d3-aee0-987c08841c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim, embed_dim, ff_dim, dropout_rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.attn = layers.MultiHeadAttention(\n",
    "            num_heads, key_dim, output_shape=embed_dim\n",
    "        )\n",
    "        self.dropout_1 = layers.Dropout(self.dropout_rate)\n",
    "        self.ln_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ffn_1 = layers.Dense(self.ff_dim, activation=\"relu\")\n",
    "        self.ffn_2 = layers.Dense(self.embed_dim)\n",
    "        self.dropout_2 = layers.Dropout(self.dropout_rate)\n",
    "        self.ln_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(\n",
    "            batch_size, seq_len, seq_len, tf.bool\n",
    "        )\n",
    "        attention_output, attention_scores = self.attn(\n",
    "            inputs,\n",
    "            inputs,\n",
    "            attention_mask=causal_mask,\n",
    "            return_attention_scores=True,\n",
    "        )\n",
    "        attention_output = self.dropout_1(attention_output)\n",
    "        out1 = self.ln_1(inputs + attention_output)\n",
    "        ffn_1 = self.ffn_1(out1)\n",
    "        ffn_2 = self.ffn_2(ffn_1)\n",
    "        ffn_output = self.dropout_2(ffn_2)\n",
    "        return (self.ln_2(out1 + ffn_output), attention_scores)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"key_dim\": self.key_dim,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "                \"ff_dim\": self.ff_dim,\n",
    "                \"dropout_rate\": self.dropout_rate,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc16c72-29c4-4e32-b68e-353d414d0d81",
   "metadata": {},
   "source": [
    "## Create Token and Position Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021887e2-9049-4530-98e9-f3de4ab4b7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, max_len, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.token_emb = layers.Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.pos_emb = layers.Embedding(input_dim=max_len, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"max_len\": self.max_len,\n",
    "                \"vocab_size\": self.vocab_size,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db7f2dd-0cca-469b-947f-717e0b57514e",
   "metadata": {},
   "source": [
    "## Build the Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e0f453-1bee-4761-8b92-dee8f7a423a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(None,), dtype=tf.int32)\n",
    "x = TokenAndPositionEmbedding(MAX_LEN, VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
    "x, attention_scores = TransformerBlock(\n",
    "    N_HEADS, KEY_DIM, EMBEDDING_DIM, FEED_FORWARD_DIM\n",
    ")(x)\n",
    "outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "gpt = models.Model(inputs=inputs, outputs=[outputs, attention_scores])\n",
    "gpt.compile(\"adam\", loss=[losses.SparseCategoricalCrossentropy(), None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53d0cb0-e0af-47ed-8524-240b12d4b12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d928b0-a0c3-4c68-86f7-3a0472e0a7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if LOAD_MODEL:\n",
    "# if True:\n",
    "    gpt.load_weights(\"./checkpoint/checkpoint.weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d911e71-4825-4e49-b563-8fe860f42d67",
   "metadata": {},
   "source": [
    "## Train the Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7900628a-2f20-43ac-ab41-7079f0ab88e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TextGenerator checkpoint\n",
    "class TextGenerator(callbacks.Callback):\n",
    "    def __init__(self, index_to_word, top_k=10):\n",
    "        self.index_to_word = index_to_word\n",
    "        self.word_to_index = {\n",
    "            word: index for index, word in enumerate(index_to_word)\n",
    "        }\n",
    "\n",
    "    def sample_from(self, probs, temperature):\n",
    "        probs = probs ** (1 / temperature)\n",
    "        probs = probs / np.sum(probs)\n",
    "        return np.random.choice(len(probs), p=probs), probs\n",
    "\n",
    "    def generate(self, start_prompt, max_tokens, temperature):\n",
    "        start_tokens = [\n",
    "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
    "        ]\n",
    "        sample_token = None\n",
    "        info = []\n",
    "        while len(start_tokens) < max_tokens and sample_token != 0:\n",
    "            x = np.array([start_tokens])\n",
    "            y, att = self.model.predict(x, verbose=0)\n",
    "            sample_token, probs = self.sample_from(y[0][-1], temperature)\n",
    "            info.append(\n",
    "                {\n",
    "                    \"prompt\": start_prompt,\n",
    "                    \"word_probs\": probs,\n",
    "                    \"atts\": att[0, :, -1, :],\n",
    "                }\n",
    "            )\n",
    "            start_tokens.append(sample_token)\n",
    "            start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
    "        print(f\"\\ngenerated text:\\n{start_prompt}\\n\")\n",
    "        return info\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.generate(\"Vatten\", max_tokens=80, temperature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a93904-cdcc-465a-98da-533cf8ec3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model save checkpoint\n",
    "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=\"./checkpoint/checkpoint.weights.h5\",\n",
    "    save_weights_only=True,\n",
    "    save_freq=\"epoch\",\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "tensorboard_callback = callbacks.TensorBoard(log_dir=\"./logs\")\n",
    "\n",
    "# Tokenize starting prompt\n",
    "text_generator = TextGenerator(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf863ea-c126-4f8c-b44c-3d0d9299aaa3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gpt.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[model_checkpoint_callback, tensorboard_callback, text_generator],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2483ae6-a4d6-442f-a434-5e6ef054141d",
   "metadata": {},
   "source": [
    "### Save the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50750062-ecf8-4652-80d5-f0bb49b2c848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "gpt.save(\"./models/gpt.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461afe63-126a-422d-9337-4446f936f0a1",
   "metadata": {},
   "source": [
    "# Text to Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739dfbf5-fe56-4c86-b190-fd5b166defd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tts = gTTS('Jag heter Johan', lang='sv', slow=True)\n",
    "tts.save('hello.mp3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd744e51-afc7-43e6-ae81-f2adf092a15d",
   "metadata": {},
   "source": [
    "# Generate Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3b398d-d22f-4edf-8fed-be3ef83bb755",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = text_generator.generate(\n",
    "    \"Meningen med livet\", max_tokens=80, temperature=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f2f456-e271-478d-85fb-fd02eaf8d1c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
